{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ad0ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Đã lưu file kết quả tại: 260504.wav ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf # Dùng để lưu file audio\n",
    "\n",
    "\n",
    "class CasualMHSA(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d_model, n_head, dropout=dropout, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        attn_mask = torch.triu(torch.ones(T, T, device=x.device) * float('-inf'), diagonal=1)\n",
    "        x_out, _ = self.mha(x, x, x, attn_mask=attn_mask)\n",
    "        return x_out\n",
    "\n",
    "class ConformerConvModule(nn.Module):\n",
    "    def __init__(self, d_model, kernel_size=15, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pointwise_conv1 = nn.Conv1d(d_model, d_model * 2, kernel_size=1)\n",
    "        self.glu = nn.GLU(dim=1)\n",
    "        self.depthwise_conv = nn.Conv1d(d_model, d_model, kernel_size, padding=(kernel_size-1)//2, groups=d_model)\n",
    "        self.batch_norm = nn.GroupNorm(num_groups=1, num_channels=d_model)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.pointwise_conv2 = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.dropout(self.pointwise_conv2(self.activation(self.batch_norm(self.depthwise_conv(self.glu(self.pointwise_conv1(x)))))))\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "class FeedForwardModule(nn.Module):\n",
    "    def __init__(self, d_model, expansion_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model, d_model*expansion_factor)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.layer2 = nn.Linear(d_model*expansion_factor, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout2(self.layer2(self.dropout1(self.activation(self.layer1(x)))))\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, kernel_size=15, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ffn1 = FeedForwardModule(d_model, dropout=dropout)\n",
    "        self.conv_module = ConformerConvModule(d_model, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.self_attn = CasualMHSA(d_model, n_head, dropout=dropout)\n",
    "        self.ffn2 = FeedForwardModule(d_model, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model); self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model); self.norm4 = nn.LayerNorm(d_model)\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x):\n",
    "        x = x + 0.5 * self.ffn1(self.norm1(x))\n",
    "        x = x + self.conv_module(self.norm2(x))\n",
    "        x = x + self.self_attn(self.norm3(x))\n",
    "        x = x + 0.5 * self.ffn2(self.norm4(x))\n",
    "        return self.final_norm(x)\n",
    "\n",
    "class AEC(nn.Module):\n",
    "    def __init__(self, d_model=128, n_fft=512, n_head=8, num_layers=4, kernel_size=15):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.n_freq = n_fft // 2 + 1\n",
    "        input_dim = self.n_freq * 4\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            ConformerBlock(d_model, n_head, kernel_size=kernel_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.mask_proj = nn.Linear(d_model, self.n_freq * 2)\n",
    "        self.vad_proj = nn.Linear(d_model, 2)\n",
    "    def forward(self, mic_stft, ref_stft):\n",
    "        B, F, T, C = mic_stft.shape\n",
    "        mic_flat = mic_stft.permute(0, 2, 1, 3).reshape(B, T, F * 2)\n",
    "        ref_flat = ref_stft.permute(0, 2, 1, 3).reshape(B, T, F * 2)\n",
    "        x = torch.cat([mic_flat, ref_flat], dim=2)\n",
    "        x = self.input_proj(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        mask = self.mask_proj(x)\n",
    "        mask = mask.view(B, T, F, 2).permute(0, 2, 1, 3)\n",
    "        mic_real, mic_imag = mic_stft[..., 0], mic_stft[..., 1]\n",
    "        mask_real, mask_imag = mask[..., 0], mask[..., 1]\n",
    "        est_real = mic_real * mask_real - mic_imag * mask_imag\n",
    "        est_imag = mic_real * mask_imag + mic_imag * mask_real\n",
    "        vad_logits = self.vad_proj(x)\n",
    "        return torch.stack([est_real, est_imag], dim=-1)\n",
    "\n",
    "# --- Các hàm hỗ trợ xử lý tín hiệu ---\n",
    "\n",
    "def run_inference(model_path, mic_path, ref_path, save_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 1. Load Model\n",
    "    model = AEC(d_model=128, n_fft=512).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # 2. Load Audio bằng Librosa\n",
    "    y_mic, _ = librosa.load(mic_path, sr=16000)\n",
    "    y_ref, _ = librosa.load(ref_path, sr=16000)\n",
    "\n",
    "    # --- SỬA LỖI TẠI ĐÂY: Căn chỉnh độ dài ---\n",
    "    min_len = min(len(y_mic), len(y_ref))\n",
    "    y_mic = y_mic[:min_len]\n",
    "    y_ref = y_ref[:min_len]\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # 3. Chuẩn bị STFT\n",
    "    n_fft, hop_length, win_length = 512, 160, 320\n",
    "    window = torch.hann_window(win_length).to(device)\n",
    "    \n",
    "    def to_stft(y):\n",
    "        y_torch = torch.from_numpy(y).float().to(device)\n",
    "        stft_complex = torch.stft(y_torch, n_fft=n_fft, hop_length=hop_length, \n",
    "                                  win_length=win_length, window=window, \n",
    "                                  center=True, return_complex=True)\n",
    "        return torch.view_as_real(stft_complex).unsqueeze(0) # (1, F, T, 2)\n",
    "\n",
    "    mic_stft = to_stft(y_mic)\n",
    "    ref_stft = to_stft(y_ref)\n",
    "\n",
    "    # 4. Forward qua model\n",
    "    with torch.no_grad():\n",
    "        # Bây giờ mic_stft và ref_stft chắc chắn có cùng số khung T\n",
    "        est_stft = model(mic_stft, ref_stft)\n",
    "\n",
    "    # 5. Inverse STFT để về miền thời gian\n",
    "    est_complex = torch.complex(est_stft[..., 0], est_stft[..., 1]).squeeze(0)\n",
    "    y_est = torch.istft(est_complex, n_fft=n_fft, hop_length=hop_length, \n",
    "                        win_length=win_length, window=window, center=True)\n",
    "    \n",
    "    # 6. Lưu kết quả\n",
    "    output_audio = y_est.cpu().numpy()\n",
    "    sf.write(save_path, output_audio, 16000)\n",
    "    print(f\"--- Đã lưu file kết quả tại: {save_path} ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MODEL_FILE = \"C:/Users/Admin.ADMIN-PC/Desktop/demo/aec_7_percent_loss/aec_v2_step_2900.pth\" # Thay bằng file của bạn\n",
    "    MIC_FILE = \"C:/Users/Admin.ADMIN-PC/Desktop/260504_mic.wav\"\n",
    "    REF_FILE = \"C:/Users/Admin.ADMIN-PC/Desktop/260504_ref.wav\"\n",
    "    SAVE_FILE = \"260504.wav\"\n",
    "    \n",
    "    run_inference(MODEL_FILE, MIC_FILE, REF_FILE, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aaad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Temp\\ipykernel_11272\\4103032781.py:9: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y1, sr1 = librosa.load(file1, sr=None)\n",
      "d:\\Neural-AEC\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nearend_mic_fileid_0.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLibsndfileError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Neural-AEC\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     y, sr_native = \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m sf.SoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Neural-AEC\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:209\u001b[39m, in \u001b[36m__soundfile_load\u001b[39m\u001b[34m(path, offset, duration, dtype)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     context = \u001b[43msf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Neural-AEC\\.venv\\Lib\\site-packages\\soundfile.py:690\u001b[39m, in \u001b[36mSoundFile.__init__\u001b[39m\u001b[34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[39m\n\u001b[32m    688\u001b[39m \u001b[38;5;28mself\u001b[39m._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[32m    689\u001b[39m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m \u001b[38;5;28mself\u001b[39m._file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode).issuperset(\u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.seekable():\n\u001b[32m    692\u001b[39m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Neural-AEC\\.venv\\Lib\\site-packages\\soundfile.py:1265\u001b[39m, in \u001b[36mSoundFile._open\u001b[39m\u001b[34m(self, file, mode_int, closefd)\u001b[39m\n\u001b[32m   1264\u001b[39m     err = _snd.sf_error(file_ptr)\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix=\u001b[33m\"\u001b[39m\u001b[33mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.name))\n\u001b[32m   1266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode_int == _snd.SFM_WRITE:\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[32m   1269\u001b[39m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[31mLibsndfileError\u001b[39m: Error opening 'nearend_mic_fileid_0.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m file2 = \u001b[33m\"\u001b[39m\u001b[33moutput_cleaned.wav\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load audio\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m y1, sr1 = \u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m y2, sr2 = librosa.load(file2, sr=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# STFT parameters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Neural-AEC\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:184\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib.PurePath)):\n\u001b[32m    181\u001b[39m     warnings.warn(\n\u001b[32m    182\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[33m\"\u001b[39m, stacklevel=\u001b[32m2\u001b[39m\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     y, sr_native = \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Neural-AEC\\.venv\\Lib\\site-packages\\decorator.py:235\u001b[39m, in \u001b[36mdecorate.<locals>.fun\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[32m    234\u001b[39m     args, kw = fix(args, kw, sig)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Neural-AEC\\.venv\\Lib\\site-packages\\librosa\\util\\decorators.py:63\u001b[39m, in \u001b[36mdeprecated.<locals>.__wrapper\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[32m     55\u001b[39m warnings.warn(\n\u001b[32m     56\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m     stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[32m     62\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Neural-AEC\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:240\u001b[39m, in \u001b[36m__audioread_load\u001b[39m\u001b[34m(path, offset, duration, dtype)\u001b[39m\n\u001b[32m    237\u001b[39m     reader = path\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     reader = \u001b[43maudioread\u001b[49m\u001b[43m.\u001b[49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[32m    243\u001b[39m     sr_native = input_file.samplerate\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Neural-AEC\\.venv\\Lib\\site-packages\\audioread\\__init__.py:126\u001b[39m, in \u001b[36maudio_open\u001b[39m\u001b[34m(path, backends)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[32m    128\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Neural-AEC\\.venv\\Lib\\site-packages\\audioread\\rawread.py:59\u001b[39m, in \u001b[36mRawAudioFile.__init__\u001b[39m\u001b[34m(self, filename)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28mself\u001b[39m._fh = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m         \u001b[38;5;28mself\u001b[39m._file = aifc.open(\u001b[38;5;28mself\u001b[39m._fh)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'nearend_mic_fileid_0.wav'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "file1 = \"nearend_mic_fileid_0.wav\"\n",
    "file2 = \"260504.wav\"\n",
    "\n",
    "# Load audio\n",
    "y1, sr1 = librosa.load(file1, sr=None)\n",
    "y2, sr2 = librosa.load(file2, sr=None)\n",
    "\n",
    "# STFT parameters\n",
    "n_fft = 512\n",
    "win_length = 320\n",
    "hop_length = 160\n",
    "\n",
    "# Compute STFT\n",
    "S1 = librosa.stft(y1, n_fft=n_fft, win_length=win_length, hop_length=hop_length)\n",
    "S2 = librosa.stft(y2, n_fft=n_fft, win_length=win_length, hop_length=hop_length)\n",
    "\n",
    "# Convert to dB\n",
    "S1_db = librosa.amplitude_to_db(np.abs(S1), ref=np.max)\n",
    "S2_db = librosa.amplitude_to_db(np.abs(S2), ref=np.max)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# -------- Audio 1 --------\n",
    "plt.subplot(4, 1, 1)\n",
    "librosa.display.waveshow(y1, sr=sr1)\n",
    "plt.title(\"Waveform - Audio 1\")\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "librosa.display.specshow(\n",
    "    S1_db,\n",
    "    sr=sr1,\n",
    "    hop_length=hop_length,\n",
    "    x_axis=\"time\",\n",
    "    y_axis=\"linear\"\n",
    ")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"STFT Spectrogram - Audio 1\")\n",
    "\n",
    "# -------- Audio 2 --------\n",
    "plt.subplot(4, 1, 3)\n",
    "librosa.display.waveshow(y2, sr=sr2)\n",
    "plt.title(\"Waveform - Audio 2\")\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "librosa.display.specshow(\n",
    "    S2_db,\n",
    "    sr=sr2,\n",
    "    hop_length=hop_length,\n",
    "    x_axis=\"time\",\n",
    "    y_axis=\"linear\"\n",
    ")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"STFT Spectrogram - Audio 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neural-AEC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
